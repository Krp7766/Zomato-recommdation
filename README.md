# ğŸ½ï¸ Zomato Restaurant Clustering using K-Means

This project performs clustering on Zomato restaurant data using **K-Means** algorithm, grouping restaurants based on their **Cost**, **Average Rating**, **Cuisines**, and **Collections**. The goal is to uncover meaningful segments in the restaurant ecosystem to assist in recommendation systems or business analysis.

---

## ğŸ“ Dataset Used

Two CSV files (originally from Kaggle or similar sources):
- `Zomato Restaurant names and Metadata.csv`: Contains restaurant names, cuisines, timings, cost, and collections.
- `Zomato Restaurant reviews.csv`: Contains user reviews with ratings for various restaurants.

---

## ğŸš€ Features Used for Clustering

- **Cost for two people** (numerically cleaned)
- **Average user rating** (aggregated from reviews)
- **Cuisines** (multi-label binarized)
- **Collections** (multi-label binarized)

---

## ğŸ§  Tech Stack

- Python
- Google Colab
- Pandas & NumPy (data wrangling)
- Seaborn & Matplotlib (visualization)
- Scikit-learn (KMeans clustering, preprocessing)

---

## ğŸ“Š Clustering Steps

1. **Data Cleaning**
   - Removed missing values in key columns (`Collections`, `Timings`, `Rating`)
   - Converted `Cost` column to numeric
   - Aggregated average ratings per restaurant from the review dataset

2. **Feature Engineering**
   - Applied `MultiLabelBinarizer` to encode `Cuisines` and `Collections`
   - Standardized features using `StandardScaler`

3. **Clustering**
   - Used **KMeans** algorithm with `n_clusters=3`
   - Clustered based on numerical and one-hot encoded features

4. **Visualization**
   - Plotted the clusters using Seaborn scatterplot with `Cost` vs `Avg_Rating`

---

## ğŸ“¸ Output Example

The final output includes a dataframe showing:
- Restaurant `Name`
- `Cost` for two people
- `Average Rating`
- List of `Cuisines`
- Assigned `Cluster` (0, 1, 2)

---

## ğŸ“ˆ Visual Output

Cluster visualization:  
Restaurants grouped by **Cost** and **Average Rating**, color-coded by cluster labels.

---

## ğŸ”§ How to Run

1. Open the [Google Colab notebook](#) *(Add link to your .ipynb file once uploaded to GitHub)*  
2. Upload the required datasets into `/content/` directory
3. Run the notebook cells sequentially

---

## ğŸ’¡ Future Enhancements

- Use **Elbow Method** or **Silhouette Score** to optimize `k`
- Integrate with a **recommendation engine**
- Deploy as a **Flask app** or **Streamlit dashboard**

---

## ğŸ“¬ Contact

Made by **Kaushal Parulekar**  
*Data Science & Big Data Analytics Enthusiast*  
ğŸ“§ Email: [kaushalrajesh200@gmail.com]  
ğŸ”— LinkedIn: [https://www.linkedin.com/in/kaushal-parulekar-82827124b/]

---

## â­ If you like this project, give it a star on GitHub!

